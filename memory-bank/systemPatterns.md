# システムパターン

## システムアーキテクチャ

本アプリケーションは、フロントエンドとバックエンドに分かれたクライアント・サーバーアーキテクチャを採用しています。

```
╭─React (Next.js)────────╮     WebSocket/SSE      ╭─FastAPI────────╮
│  React Flow            │◀──────────────────────▶│  LLM Service    │
│  XState (branch state) │                        │  (OpenAI / ...) │
╰──────────────┬────────╯                        ╰────────┬──────╯
               │ SQL/Graph                             │
               ▼                                       ▼
           PostgreSQL                           Ollama / OpenAI
```

*   **フロントエンド**: Next.js (React) を使用し、UIの描画にはReact Flow、状態管理にはZustand/Recoil、スタイリングにはTailwind CSS (+ shadcn/ui) を利用します。会話のブランチ状態管理にはXStateの導入が検討されています。
*   **バックエンド**: Python (FastAPI) を使用し、LLMサービス（OpenAI / Ollamaなど）との連携を担当します。
*   **データベース**: PostgreSQLを使用し、会話履歴やブランチ情報を永続化します。

## 主要な技術的決定とデザインパターン

*   **会話ブランチのロジック**:
    *   メッセージは`Messages`テーブルに保存されます。
        *   `id` (PK)
        *   `parent_id` (NULLならroot)
        *   `role` (user/assistant/system)
        *   `content`
        *   `created_at`
    *   ブランチを選択した場合、選択されたノードから根まで`parent_id`をたどってメッセージを逆順に並べます。
    *   LLMに投げる際には、`max_tokens`を超えないように古いメッセージからtruncateされます。
    *   新しい問い直しは、現在選択されているノードを親として新しいメッセージをINSERTすることで実現されます。これにより、どの枝からでも無限に分岐が可能です（Gitの`checkout -b`のような挙動）。
*   **リアルタイム通信**: フロントエンドとバックエンド間の通信にはWebSocketまたはSSE (Server-Sent Events) を利用し、LLMからの応答をリアルタイムでUIに反映させます。
*   **データベースマイグレーション**: Alembicを使用してデータベーススキーマの変更を管理します。

## コンポーネント間の関係

*   **フロントエンド ↔ バックエンド**: WebSocket/SSEを通じてリアルタイムでチャットメッセージとLLM応答をやり取りします。
*   **バックエンド ↔ LLMサービス**: バックエンドのLLMサービスが、設定されたLLMプロバイダー（OpenAI, Ollamaなど）のAPIを呼び出します。
*   **バックエンド ↔ データベース**: バックエンドがPostgreSQLに対してメッセージの保存、取得、更新を行います。

## 重要な実装パス

*   **メッセージの保存と取得**: ユーザー入力とLLM応答を正確にデータベースに保存し、選択されたブランチに基づいて関連する会話履歴を効率的に取得するロジック。
*   **ツリー構造の構築と表示**: データベースから取得した`parent_id`情報を用いて、フロントエンドで会話のツリー構造を正確に再構築し、React Flowで視覚的に表現する。
*   **LLMコンテキスト管理**: 選択されたブランチのコンテキストを正確に抽出し、`max_tokens`の制約内でLLMに渡すためのロジック。
